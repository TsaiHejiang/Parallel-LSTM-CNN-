{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "########### This is the codes for ML-JI ###############\n",
    "#######################################################\n",
    "\n",
    "### Languages\n",
    "#The python version is python37\n",
    "#The tensorflow vesion is tensorflow1.14\n",
    "#Please check and install the other libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os, logging, pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy import signal\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, regularizers\n",
    "## Ignore all the warnings\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['KMP_WARNINGS'] = '0'\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, for LSTM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing functions.\n",
    "#read, wrap and normalize the data\n",
    "\n",
    "\n",
    "def get_station_data(fname):\n",
    "    \"\"\"\n",
    "    Obtain the pandas dataframe from dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(fname)\n",
    "    dataset[\"date\"] = pd.to_datetime(dataset['date'], dayfirst=True,errors='ignore')\n",
    "    #dataset = dataset.replace(-99, np.NaN)\n",
    "    dataset = dataset.set_index(\"date\")\n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    # This date range is a fake date that helps better arrange the data.\n",
    "    # This can be changed with orders\n",
    "    dataset = dataset.loc[pd.date_range(start='1/1/1980', end='28/10/2009')]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_wrapped_data(dataset, wrap_length=214):\n",
    "    \"\"\"\n",
    "    Wrap the data for the shape requirement of LSTM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: the pandas dataframe obtained from the function get_station_data().\n",
    "    wrap_length: the number of time steps to be considered for the LSTM layer.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    data_x_dict: the input dictionary whose key is the date and value is the corresponding wrapped input matrix of each sample.\n",
    "    data_y_dict: the output dictionary whose key is the date and value is the corresponding target of each sample.\n",
    "    \"\"\"\n",
    "    data_x_dict, data_y_dict = {}, {}\n",
    "\n",
    "    for date_i in tqdm(dataset.index, desc=f'Prep aring data with wrap length = {wrap_length}'):\n",
    "        try:\n",
    "            \n",
    "            #adjusting input and output features by choosing the columns here, also in split_train_test\n",
    "            \n",
    "            data_x = dataset.loc[pd.date_range(end=date_i,\n",
    "                                               periods=wrap_length + 1,\n",
    "                                               freq=\"d\")[:-1], [\"Prep\",\"Prep_5\",\"Prep_10\",\"Prep_15\"],\n",
    "                                ].to_numpy(dtype='float16')\n",
    "            data_y = dataset.loc[pd.date_range(end=date_i,\n",
    "                                               periods=wrap_length + 1,\n",
    "                                               freq=\"d\")[-1:], [\"w1\",\"w2\",\"w3\",\"w4\",\"w5\",\"w6\",\"w7\",\"w8_1\",\"w8_2\",\"w8_3\",\"w8_4\",\"w9\",\"w10\",\"w11\",\"w12_1\",\"w12_2\",\"w12_3\",\"w12_4\"],\n",
    "                                ].to_numpy(dtype='float16')\n",
    "\n",
    "            data_x_dict[date_i] = data_x\n",
    "            data_y_dict[date_i] = data_y\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return data_x_dict, data_y_dict\n",
    "\n",
    "\n",
    "def split_train_test(dataset, data_x_dict, data_y_dict, frac=0.8, random_state=100, scale=True):\n",
    "    \"\"\"\n",
    "    Randomly split the dataset for training and testing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: the pandas dataframe obtained from the function get_station_data().\n",
    "    data_x_dict: the input dictionary obtained from the function get_wrapped_data().\n",
    "    data_y_dict: the output dictionary obtained from the function get_wrapped_data().\n",
    "    frac: the fraction of samples to be trained.\n",
    "    random_state: the random seed (default: 100).\n",
    "    scale: [bool] whether scale the split dataset by the mean and std values of the training data (default: True).\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    train_dates: the dates of the picked training data.\n",
    "    test_dates: the dates of the picked testing data.\n",
    "    train_x: the (scaled) inputs for training.\n",
    "    train_y: the (scaled) outputs for training.\n",
    "    test_x: the (scaled) inputs for testing.\n",
    "    test_y: the (scaled) outputs for testing.\n",
    "    scale_params: the mean and std values of the training data (available when scale is True)\n",
    "    \"\"\"\n",
    "    train_dates = (dataset.loc[data_x_dict.keys()].sample(frac=frac, random_state=random_state).index)\n",
    "    test_dates  = dataset.loc[data_x_dict.keys()].drop(train_dates).index\n",
    "\n",
    "    train_x = np.stack([data_x_dict.get(i) for i in train_dates.to_list()])\n",
    "    train_y = np.stack([data_y_dict.get(i) for i in train_dates.to_list()])\n",
    "    test_x  = np.stack([data_x_dict.get(i) for i in test_dates.to_list()])\n",
    "    test_y  = np.stack([data_y_dict.get(i) for i in test_dates.to_list()])\n",
    "\n",
    "    scale_params = {\"train_x_mean\": 0, \"train_x_std\": 1, \"train_y_mean\": 0, \"train_y_std\": 1}\n",
    "\n",
    "    if scale is False:\n",
    "        return train_dates, test_dates, train_x, train_y, test_x, test_y, scale_params\n",
    "    else:\n",
    "        scale_params[\"train_x_mean\"] = (dataset.loc[train_dates, [\"Prep\",\"Prep_5\",\"Prep_10\",\"Prep_15\"]].mean().values)\n",
    "        scale_params[\"train_x_std\"]  = (dataset.loc[train_dates, [\"Prep\",\"Prep_5\",\"Prep_10\",\"Prep_15\"]].std().values)\n",
    "        scale_params[\"train_y_mean\"] = dataset.loc[train_dates, [\"w1\",\"w2\",\"w3\",\"w4\",\"w5\",\"w6\",\"w7\",\"w8_1\",\"w8_2\",\"w8_3\",\"w8_4\",\"w9\",\"w10\",\"w11\",\"w12_1\",\"w12_2\",\"w12_3\",\"w12_4\"]].mean().values\n",
    "        scale_params[\"train_y_std\"]  = dataset.loc[train_dates, [\"w1\",\"w2\",\"w3\",\"w4\",\"w5\",\"w6\",\"w7\",\"w8_1\",\"w8_2\",\"w8_3\",\"w8_4\",\"w9\",\"w10\",\"w11\",\"w12_1\",\"w12_2\",\"w12_3\",\"w12_4\"]].std().values\n",
    "\n",
    "        train_x = (train_x - scale_params[\"train_x_mean\"][None, None, :]) / scale_params[\"train_x_std\"][None, None, :]\n",
    "        train_y = (train_y - scale_params[\"train_y_mean\"][None, :]) / scale_params[\"train_y_std\"][None, :]\n",
    "        test_x  = (test_x - scale_params[\"train_x_mean\"][None, None, :]) / scale_params[\"train_x_std\"][None, None, :]\n",
    "        test_y  = (test_y - scale_params[\"train_y_mean\"][None, :]) / scale_params[\"train_y_std\"][None, :]\n",
    "\n",
    "        return train_dates, test_dates, train_x, train_y, test_x, test_y, scale_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_PATH  = r\"C:\\Users\\e0427809\\Desktop\\Try\\well_50\"\n",
    "\n",
    "####################\n",
    "#   Basin set up   #\n",
    "####################\n",
    "STATION_ID = 'merged_wells_50_CNN' # USGS code used in the MOPEX dataset\n",
    "\n",
    "####################\n",
    "#  Hyperparameters #\n",
    "####################\n",
    "RANDOM_SEED   = 100        \n",
    "WRAP_LENGTH   = 214        # Timestep of the LSTM model\n",
    "TRAIN_FRAC    = 0.8        # The fraction of spliting traning and testing dataset\n",
    "\n",
    "LEARNING_RATE = 0.03\n",
    "EPOCH_NUMBER  = 300 # from 200 to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the input and output should be in the same file here.\n",
    "read_path = os.path.join(WORKING_PATH, f'{STATION_ID}.csv')\n",
    "#you need to create a folder \"data\" to save the wrapped and normalized data \n",
    "data_path  = os.path.join(WORKING_PATH, 'data', f'{STATION_ID}_data.pickle')\n",
    "#you need to create a folder \"results\" and a sub-folder \"model\" to save the trained ML model. \n",
    "model_path = os.path.join(WORKING_PATH, 'results', 'model', f'{STATION_ID}_{RANDOM_SEED}_keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodata = get_station_data(fname=read_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data_x_dict, data_y_dict = pickle.load(f)\n",
    "else:\n",
    "    data_x_dict, data_y_dict = get_wrapped_data(dataset=hydrodata,  wrap_length=WRAP_LENGTH)\n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump([data_x_dict, data_y_dict], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results = split_train_test(dataset=hydrodata, \n",
    "                                       data_x_dict=data_x_dict, \n",
    "                                       data_y_dict=data_y_dict, \n",
    "                                       frac=TRAIN_FRAC, \n",
    "                                       random_state=RANDOM_SEED, \n",
    "                                       scale=True)\n",
    "\n",
    "train_dates, test_dates, x_train, y_train, x_test, y_test, scale_params = split_results\n",
    "\n",
    "print(f'The shape of x_train, y_train after wrapping by {WRAP_LENGTH} days are {x_train.shape}, {y_train.shape}')\n",
    "print(f'The shape of x_test, y_test after wrapping by {WRAP_LENGTH} days are   {x_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results = split_train_test(dataset=hydrodata, \n",
    "                                       data_x_dict=data_x_dict, \n",
    "                                       data_y_dict=data_y_dict, \n",
    "                                       frac=TRAIN_FRAC, \n",
    "                                       random_state=RANDOM_SEED, \n",
    "                                       scale=True)\n",
    "\n",
    "train_dates, test_dates, x_train, y_train, x_test, y_test, scale_params = split_results\n",
    "\n",
    "print(f'The shape of x_train, y_train after wrapping by {WRAP_LENGTH} days are {x_train.shape}, {y_train.shape}')\n",
    "print(f'The shape of x_test, y_test after wrapping by {WRAP_LENGTH} days are   {x_test.shape}, {y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, for the Spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\e0427809\\Desktop\\Try\\Spatial\\spatial_features.csv', header=None, index_col=False)\n",
    "df_normalized = pd.DataFrame()\n",
    "\n",
    "# Iterate over each column for Z-score normalization\n",
    "for column in df.columns:\n",
    "    data = df[column]\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    normalized_data = (data - mean) / std\n",
    "    df_normalized[column] = normalized_data\n",
    "\n",
    "df_normalized.to_csv(r'C:\\Users\\e0427809\\Desktop\\Try\\Spatial\\normed.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = pd.read_csv(r'C:\\Users\\e0427809\\Desktop\\Try\\Spatial\\normed.csv', header=None, index_col=False)\n",
    "#Make sure the data is nomlized.\n",
    "data = sp.values\n",
    "\n",
    "# Reshape the data to fit the input requirements of the CNN model.\n",
    "data_cnn_reshape = data.reshape(18, 32, 1)\n",
    "\n",
    "#repeat the data to make sure it Consistent with LSTM data so that the model can run.\n",
    "#repeating numbers should be changed regarding the shapes of data for LSTM model.\n",
    "x_train_cnn = np.repeat(data_cnn_reshape, 8544, axis=0)\n",
    "data_cnn = np.reshape(x_train_cnn, (8544, 18, 32, 1))\n",
    "x_test_cnn = np.repeat(data_cnn_reshape, 2136, axis=0)\n",
    "test_cnn = np.reshape(x_test_cnn, (2136, 18, 32, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are loss functions for choosing.\n",
    "#Not sure about the issues with weighted_rmse_1 and weighted_rmse_2\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    mse = tf.keras.losses.mean_squared_error(y_true, y_pred)  \n",
    "    return tf.reduce_mean(mse)\n",
    "def mean_r2(y_true, y_pred):\n",
    "    SS_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)  \n",
    "    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)  \n",
    "    r2 = 1 - SS_res / (SS_tot + tf.keras.backend.epsilon())  \n",
    "    return tf.reduce_mean(r2) \n",
    "\n",
    "def nse_loss(y_true, y_pred):\n",
    "    epsilon = 1e-7  \n",
    "    y_pred_smoothed = tf.where(tf.equal(y_pred, 0), y_pred + epsilon, y_pred)\n",
    "    numerator = tf.reduce_sum(tf.square(y_true - y_pred_smoothed), axis=0)\n",
    "    denominator = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "    nse = 1 - (numerator / denominator)\n",
    "    return tf.reduce_mean(1-nse)\n",
    "\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    mse = tf.keras.losses.mean_squared_error(y_true, y_pred)  \n",
    "    return tf.reduce_mean(mse)\n",
    "\n",
    "\n",
    "def weighted_rmse_1(y_true, y_pred):\n",
    "    threshold = 4.0  \n",
    "    weight_high = 3.0  \n",
    "    weight_low = 0.5 \n",
    "\n",
    "    weights = K.switch(y_true > threshold, K.ones_like(y_true) * weight_high, K.ones_like(y_true) * weight_low)  \n",
    "\n",
    "    mse = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "    weighted_mse = mse * weights  \n",
    "\n",
    "    return K.mean(K.sqrt(weighted_mse))\n",
    "\n",
    "def weighted_rmse_2(y_true, y_pred):\n",
    "    threshold = 4.0  \n",
    "    weight_high = 3.0  \n",
    "    weight_low = 0.5 \n",
    "    \n",
    "    weights = K.ones_like(y_true) * weight_low  # Initialize weights as weight_low for all elements\n",
    "    \n",
    "    high_indices = tf.cast(y_true > threshold, dtype=K.floatx())  # Create a tensor of 1.0 for high indices and 0.0 for others\n",
    "    weights = weights * (1.0 - high_indices) + high_indices * weight_high  # Element-wise multiplication and addition\n",
    "    \n",
    "    mse = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "    weighted_mse = mse * weights\n",
    "    \n",
    "    return K.mean(K.sqrt(weighted_mse))\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Filter out values where y_true is less than or equal to 4\n",
    "    mask = tf.greater(y_true, 4.0)\n",
    "    filtered_true = tf.boolean_mask(y_true, mask)\n",
    "    filtered_pred = tf.boolean_mask(y_pred, mask)\n",
    "    \n",
    "    # Check if filtered tensors are empty\n",
    "    if tf.size(filtered_true) == 0:\n",
    "        return 0.0  # Return 0 if no valid values are available\n",
    "    \n",
    "    # Calculate RMSE only for filtered values\n",
    "    mse = K.mean(K.square(filtered_true - filtered_pred), axis=-1)\n",
    "    rmse = K.sqrt(mse)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kge_loss(y_true, y_pred):\n",
    "    # Split y_pred into observed and modeled data\n",
    "    observed_data = y_true\n",
    "    modeled_data = y_pred\n",
    "\n",
    "    # Compute the correlation coefficient between observed and modeled data\n",
    "    observed_mean = tf.reduce_mean(observed_data)\n",
    "    modeled_mean = tf.reduce_mean(modeled_data)\n",
    "    observed_std = tf.math.reduce_std(observed_data)\n",
    "    modeled_std = tf.math.reduce_std(modeled_data)\n",
    "\n",
    "    centered_observed = observed_data - observed_mean\n",
    "    centered_modeled = modeled_data - modeled_mean\n",
    "\n",
    "    r = tf.reduce_mean(centered_observed * centered_modeled) / (observed_std * modeled_std)\n",
    "\n",
    "    # Compute the mean ratio (β) and standard deviation ratio (γ)\n",
    "    mean_ratio = modeled_mean / observed_mean\n",
    "    std_ratio = modeled_std / observed_std\n",
    "\n",
    "    # Compute the KGE\n",
    "    kge = 1 - tf.sqrt((r - 1) ** 2 + (mean_ratio - 1) ** 2 + (std_ratio - 1) ** 2)\n",
    "\n",
    "    return -kge  # Negate the KGE to be minimized as a loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now set up the Parallel LSTM+CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cnn = layers.Input(shape=(18, 32, 1), name='input_cnn')\n",
    "cnn = layers.Conv2D(filters=3, kernel_size=(3,3), activation='relu')(input_cnn)\n",
    "cnn = layers.MaxPooling2D(pool_size=(3, 3))(cnn)\n",
    "cnn = layers.Flatten()(cnn)\n",
    "dense_cnn = layers.Dense(units=18, activation='relu', name='dense_cnn')(cnn)\n",
    "\n",
    "input_lstm = layers.Input(x_train.shape[1:], name='input_lstm')\n",
    "lstm = layers.LSTM(units=60, name='lstm',\n",
    "                   kernel_regularizer=regularizers.l2(0.001),\n",
    "                   recurrent_regularizer=regularizers.l2(0.001))(input_lstm)\n",
    "\n",
    "concatenated = layers.concatenate([dense_cnn, lstm])\n",
    "\n",
    "output = layers.Dense(units=18, name='dense_output', activation='linear', use_bias=False, \n",
    "                      kernel_regularizer=regularizers.l2(0.001))(concatenated)\n",
    "output = layers.Reshape(target_shape=(1, 18))(output)\n",
    "model = models.Model(inputs=[input_cnn, input_lstm], outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es     = callbacks.EarlyStopping(monitor='val_R2', mode='max', verbose=1, patience=30, \n",
    "                                 min_delta=0.01, restore_best_weights=True)\n",
    "reduce = callbacks.ReduceLROnPlateau(monitor='val_R2', factor=0.5, patience=15, verbose=1, \n",
    "                                     mode='max', min_delta=0.01, cooldown=0, min_lr=LEARNING_RATE / 100)\n",
    "tnan   = callbacks.TerminateOnNaN()\n",
    "\n",
    "model.compile(loss = mse_loss, metrics=[mean_r2], optimizer=optimizers.Adam(lr=LEARNING_RATE))\n",
    "model.fit([data_cnn,x_train], y_train, epochs=EPOCH_NUMBER, batch_size=214, validation_split=0.2, \n",
    "          callbacks=[es, reduce, tnan])\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict([data_cnn,x_train], batch_size=214)\n",
    "pred_test  = model.predict([test_cnn,x_test], batch_size=214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_train = pred_train * scale_params['train_y_std'] + scale_params['train_y_mean']\n",
    "pr_test = pred_test  * scale_params['train_y_std'] + scale_params['train_y_mean']\n",
    "\n",
    "pr_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating index\n",
    "\n",
    "def cal_nse(obs, sim):\n",
    "\n",
    "    # compute numerator and denominator\n",
    "    numerator   = np.nansum((obs - sim)**2)\n",
    "    denominator = np.nansum((obs - np.nanmean(obs))**2)\n",
    "    # compute coefficient\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "def R2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "\n",
    "def calculate_rmse(observed, predicted):\n",
    "    mse = np.mean((observed - predicted)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodata.loc[train_dates, ['flow_pred_w1']] = pr_train[:,0,0].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w2']] = pr_train[:,0,1].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w3']] = pr_train[:,0,2].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w4']] = pr_train[:,0,3].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w5']] = pr_train[:,0,4].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w6']] = pr_train[:,0,5].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w7']] = pr_train[:,0,6].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w8_1']] = pr_train[:,0,7].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w8_2']] = pr_train[:,0,8].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w8_3']] = pr_train[:,0,9].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w8_4']] = pr_train[:,0,10].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w9']] = pr_train[:,0,11].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w10']] = pr_train[:,0,12].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w11']] = pr_train[:,0,13].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w12_1']] = pr_train[:,0,14].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w12_2']] = pr_train[:,0,15].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w12_3']] = pr_train[:,0,16].reshape(-1, 1)\n",
    "hydrodata.loc[train_dates, ['flow_pred_w12_4']] = pr_train[:,0,17].reshape(-1, 1)\n",
    "##Training part\n",
    "hydrodata.to_csv(r'C:\\Users\\e0427809\\Desktop\\Try\\well_50\\results\\Parallel_mse_loss_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodata.loc[test_dates, ['flow_pred_w1']] = pr_test[:,0,0].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w2']] = pr_test[:,0,1].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w3']] = pr_test[:,0,2].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w4']] = pr_test[:,0,3].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w5']] = pr_test[:,0,4].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w6']] = pr_test[:,0,5].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w7']] = pr_test[:,0,6].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w8_1']] = pr_test[:,0,7].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w8_2']] = pr_test[:,0,8].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w8_3']] = pr_test[:,0,9].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w8_4']] = pr_test[:,0,10].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w9']] = pr_test[:,0,11].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w10']] = pr_test[:,0,12].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w11']] = pr_test[:,0,13].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w12_1']] = pr_test[:,0,14].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w12_2']] = pr_test[:,0,15].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w12_3']] = pr_test[:,0,16].reshape(-1, 1)\n",
    "hydrodata.loc[test_dates, ['flow_pred_w12_4']] = pr_test[:,0,17].reshape(-1, 1)\n",
    "##Training part + Testing part\n",
    "##Which is the final out puts\n",
    "hydrodata.to_csv(r'C:\\Users\\e0427809\\Desktop\\Try\\well_50\\results\\Parallel_mse_loss.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###select the extraction windows\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\e0427809\\Desktop\\Try\\well_50\\results\\Plot\\give\\4-inputs_mse\\4-inputs_MSE_LOSS_Training.csv')\n",
    "df.loc[df['Obs_W1'] < 1, 1:] = 0\n",
    "columns_to_keep = ['Obs_W1', 'Sim_W1', '2*Sim_W1']  \n",
    "df = df[columns_to_keep]\n",
    "df.to_csv(r'C:\\Users\\e0427809\\Desktop\\Try\\well_50\\results\\Plot\\give\\4-inputs_mse\\W1_4-inputs_EXTRACTIONS_MSE_Training.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluations\n",
    "df = pd.read_csv(r'C:\\Users\\e0427809\\Desktop\\Try\\well_50\\results\\Plot\\give\\4-inputs_mse\\W12_4_4-inputs_EXTRACTIONS_MSE_Training.csv')\n",
    "df2 = pd.read_csv(r'C:\\Users\\e0427809\\Desktop\\Try\\well_50\\results\\Plot\\give\\4-inputs_mse\\W12_4_4-inputs_EXTRACTIONS_MSE_Testing.csv')\n",
    "# Extract data from two columns\n",
    "observed_data = df['Obs_W12_4'].values\n",
    "# simulated_data = df['Sim_W2'].values\n",
    "simulated_data_2 = df['2*Sim_W12_4'].values\n",
    "\n",
    "\n",
    "ob_t = df2['Obs_W12_4'].values\n",
    "sim_t = df2['2*Sim_W12_4'].values\n",
    "\n",
    "# # Calculate KGE score\n",
    "# kge_score = calculate_kge(observed_data, simulated_data_2)\n",
    "# print(\"KGE score:\", kge_score)\n",
    "NSE_score = cal_nse(observed_data, simulated_data_2)\n",
    "print(\"NSE_score:\", NSE_score)\n",
    "\n",
    "\n",
    "Test_NSE = cal_nse(ob_t, sim_t)\n",
    "print(\"Test_NSE:\", Test_NSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot the results\n",
    "\n",
    "hydrodata= pd.read_csv(r'C:\\Users\\e0427809\\Desktop\\Try\\well_50\\results\\Plot\\scenario3_extractionwindow.csv')\n",
    "plot_set1 = pd.to_datetime(hydrodata['date'])\n",
    "#plt.plot(plot_set1, hydrodata['flow_pred_w12_1'],label=\"simulated_rate\", color = '#e41a1c', ls='--')\n",
    "plt.plot(plot_set1, hydrodata['w1'], label=\"Observed\", color='#377eb8',lw='1.8')\n",
    "plt.plot(plot_set1, hydrodata['2flow_pred_w1'], label=\"Simulated\", color='#e41a1c', ls='--')\n",
    "plt.title(f'W1')\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=10))\n",
    "plt.gcf().autofmt_xdate()\n",
    "#plt.ylim([0, 8500])\n",
    "plt.legend(fontsize = 10)\n",
    "# plt.text(0.05, 0.95, 'RMSE_Train: 0.684', ha='left', va='top', transform=plt.gca().transAxes)\n",
    "# plt.text(0.05, 0.85, 'RMSE_Test: 0.882', ha='left', va='top', transform=plt.gca().transAxes)\n",
    "plt.savefig(r'C:\\Users\\e0427809\\Desktop\\Try\\well_50\\results\\Plot\\second\\W1.png')\n",
    "plt.savefig(r'C:\\Users\\e0427809\\Desktop\\Try\\well_50\\results\\Plot\\second\\W1.tiff')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
